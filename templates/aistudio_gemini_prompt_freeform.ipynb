{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "kWIuwKG2_oWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b01aa2e-6e0c-45b0-d3d9-5f0a35a56c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "    # The SDK will automatically read it from the GOOGLE_API_KEY environment variable.\n",
        "    # In Colab get the key from Colab-secrets (\"ğŸ”‘\" in the left panel).\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Parse the arguments\n",
        "\n",
        "model = 'gemini-1.5-pro' # @param {isTemplate: true}\n",
        "contents_b64 = 'W10=' # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MSwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9' # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False\n",
        "\n",
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"ğŸ“ Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocd6brCaYgCM",
        "outputId": "e27838b7-c315-491b-e926-f4041458b383"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the video and print a confirmation.\n",
        "video_file_name = \"/content/drive/MyDrive/äººå·¥æ™ºèƒ½/å›½äº§AIå¤§æ¨¡å‹ DeepSeekV3 æ ¸å¿ƒæŠ€æœ¯è¯¦è§£ï¼DeepSeekè®­ç»ƒæ–¹æ³•ä¾¿å®œåœ¨å“ªï¼ŸMLAæ˜¯ä»€ä¹ˆï¼ŸMoEæŠ€æœ¯ä¼šæˆä¸ºå¤§æ¨¡å‹çš„ä¸»æµæŠ€æœ¯ï¼Ÿå¤§æ¨¡å‹å¾®è°ƒ.mp4\"\n",
        "\n",
        "print(f\"Uploading file...\")\n",
        "video_file = genai.upload_file(path=video_file_name)\n",
        "print(f\"Completed upload: {video_file.uri}\")\n",
        "import time\n",
        "\n",
        "# Check whether the file is ready to be used.\n",
        "while video_file.state.name == \"PROCESSING\":\n",
        "    print('.', end='')\n",
        "    time.sleep(10)\n",
        "    video_file = genai.get_file(video_file.name)\n",
        "\n",
        "if video_file.state.name == \"FAILED\":\n",
        "  raise ValueError(video_file.state.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCEHJ-l8Yk-3",
        "outputId": "baf9b663-a7ee-4e0d-bfd9-b9a6d4f33eb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading file...\n",
            "Completed upload: https://generativelanguage.googleapis.com/v1beta/files/15mmvbtmde0q\n",
            "..........."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zAD69vE92b"
      },
      "source": [
        "## Call `generate_content`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LB2LxPmAB95V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be0a16c7-687c-48df-e19c-d47d82e53ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making LLM inference request...\n",
            "Successfully generating reponse:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "å¥½çš„ï¼Œæˆ‘å¾ˆä¹æ„å¸®åŠ©ä½ ã€‚ä»¥ä¸‹æ˜¯è§†é¢‘ä¸­æå‡ºçš„é—®é¢˜çš„è§£ç­”ã€‚\n\n**1. DeepseekV3çš„è®­ç»ƒç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ**\nDeepseek V3 è®­ç»ƒæ—¶ä½¿ç”¨ FP8 æ··åˆç²¾åº¦æ¨¡å‹ã€‚å®ƒæˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œåœ¨ H800 GPU ä¸Šè®­ç»ƒæ•´å¥—æ¨¡å‹ä»…éœ€ 557 ä¸‡ç¾å…ƒï¼Œä½¿ç”¨è¾ƒè€ç‰ˆæœ¬çš„ H800 GPUï¼ŒèŠ±è´¹çº¦äººæ°‘å¸ 4000 ä¸‡å…ƒã€‚\n\n**2. è¯·åˆ—å‡ºDeepseekV3çš„ä¸»è¦åˆ›æ–°ç‚¹**\nDeepseek V3çš„ä¸»è¦åˆ›æ–°ç‚¹æœ‰ï¼š\n* Multi-Head Latent Attention (MLA)\n* DeepSeekMoE\n* Tokené¢„æµ‹ï¼ˆMulti-Token Predictionï¼‰\n* FP8è®­ç»ƒ\n\n**3. åœ¨DeepseekV3åšæ¨ç†æ—¶ï¼Œå®é™…ä¸Šæ¯ä¸ªtokenåªæœ‰å¤šå°‘å‚æ•°è¢«æ¿€æ´»ï¼Ÿ**\nåœ¨ DeepseekV3 åšæ¨ç†æ—¶ï¼Œæ¯ä¸ª token åªæ¿€æ´» 37B çš„å‚æ•°ã€‚\n\n**4. DeepSeek-V3ä½¿ç”¨äº†å¤šå¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Ÿ**\nDeepSeek-V3ä½¿ç”¨äº† 128k å¤§å°çš„ä¸Šä¸‹æ–‡çª—å£ã€‚\n\n**5. å¤§æ¨¡å‹æ¨ç†æ—¶å€™å“ªä¸¤ä¸ªæŒ‡æ ‡å¾ˆé‡è¦ï¼Ÿå®ƒä»¬åˆ†åˆ«è¡¡é‡äº†ä»€ä¹ˆï¼Ÿ**\nå¤§æ¨¡å‹æ¨ç†æ—¶ï¼Œä¸¤ä¸ªé‡è¦æŒ‡æ ‡æ˜¯ TFFTï¼ˆTime-to-First-Tokenï¼‰å’Œ TPOTï¼ˆTime-Per-Output-Tokenï¼‰ï¼Œåˆ†åˆ«è¡¡é‡é¦–ä¸ª token çš„ç”Ÿæˆæ—¶é—´å’Œæ¯ä¸ª token çš„ç”Ÿæˆæ—¶é—´ã€‚\n\n**6. DeepseekV3çš„æ¨ç†é˜¶æ®µä½¿ç”¨äº†ä»€ä¹ˆæ¶æ„ï¼Ÿè¯·ä»‹ç»è¿™ä¸ªæ¶æ„çš„ç‰¹ç‚¹å’Œç­–ç•¥ã€‚**\nDeepseekV3æ¨ç†é˜¶æ®µä½¿ç”¨äº† Prefill å’Œ Decode åˆ†ç¦»çš„æ¶æ„ã€‚Prefill é˜¶æ®µçš„ token ç”Ÿæˆå¯å¹¶è¡ŒåŒ–ï¼Œæ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹é˜¶æ®µï¼ŒPre-training è®¡ç®—å ä¸»å¯¼åœ°ä½ã€‚Decode é˜¶æ®µçš„ token ç”Ÿæˆæ˜¯é€ä¸ªç”Ÿæˆçš„ï¼Œæ˜¯ä¸€ä¸ªå†…å­˜å¯†é›†å‹é˜¶æ®µã€‚åœ¨ Prefill é˜¶æ®µï¼ŒKV å€¼æ˜¯å‹ç¼©åè®¡ç®—çš„ï¼Œå¹¶ç¼“å­˜åœ¨æ˜¾å­˜ä¸­ï¼Œè€Œ Decode é˜¶æ®µç›´æ¥å–æ˜¾å­˜é‡Œçš„å‹ç¼©å KV å€¼ï¼Œä¸éœ€è¦å†è®¡ç®—ï¼Œå› æ­¤å¤§å¤§é™ä½äº†æ˜¾å­˜å ç”¨å’Œè®¡ç®—é‡ã€‚\n\n**7. DeepseekV3åœ¨Prefillé˜¶æ®µçš„Attentionéƒ¨åˆ†ä¸ºä»€ä¹ˆè¦è®¾ç½®è¾ƒå°çš„TPæ•°é‡4ï¼Ÿ**\nå› ä¸º Prefill é˜¶æ®µæœ¬èº«æ˜¯è®¡ç®—å¯†é›†å‹çš„ï¼ŒGPU åˆ©ç”¨ç‡é«˜ï¼Œå¦‚æœ TP è®¾ç½®è¿‡å¤§ä¼šå¯¼è‡´é€šä¿¡å¼€é”€è¿‡å¤§ã€‚ä¸ºäº†å¹³è¡¡è®¡ç®—ä¸é€šä¿¡ï¼Œå°† TP è®¾ç½®ä¸ºè¾ƒå°çš„ 4ã€‚\n\n**8. è¯·ä»‹ç»Prefillé˜¶æ®µæ‰€ä½¿ç”¨çš„å†—ä½™ä¸“å®¶çš„æ¦‚å¿µã€‚**\nPrefill é˜¶æ®µä½¿ç”¨äº†å†—ä½™ä¸“å®¶æœºåˆ¶ï¼Œåœ¨æ¯ä¸ªå¡ä¸Šéƒ½éƒ¨ç½²ä¸€ä»½ä¸“å®¶ï¼Œä»¥å‡å°‘æ•°æ®æ¬è¿ã€‚\n\n**9. è¯·è§£é‡Šdevice-limited routingæ˜¯æ€ä¹ˆè¿ä½œçš„ï¼Ÿæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ**\nDevice-limited Routing æ˜¯ Deepseek V2 é¦–åˆ›ï¼Œåœ¨æ¯ä¸ª token éœ€è¦æ¿€æ´»çš„ M ä¸ª experts å½“ä¸­ï¼Œé€‰å–åˆ†æœ€é«˜çš„ M ä¸ªè®¾å¤‡ä¸Šçš„ expertï¼Œå¹¶ä¸”åªåœ¨é€‰ä¸­çš„ M ä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—ï¼Œå‡å°‘äº†è·¨è®¾å¤‡çš„é€šä¿¡æˆæœ¬ã€‚\n\n**10. MLAåœ¨MHAä¸Šåšäº†ä»€ä¹ˆæ ·çš„æ”¹è¿›ï¼Ÿæ˜¯ä¸ºäº†è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ**\nåœ¨MHA ä¸­ï¼Œå¯¹äºæ¯ä¸€ä¸ª tokenï¼Œéƒ½éœ€è¦ä¿å­˜ 2N<sub>h</sub>d<sub>h</sub>l çš„ KV å€¼ï¼Œå¦‚æœå±‚æ•° L å¾ˆå¤§ï¼ˆä¾‹å¦‚ L=70ï¼‰ï¼Œé‚£ä¹ˆç¼“å­˜å¤§å°ä¸º 2N<sub>h</sub>d<sub>h</sub>lLï¼Œç¼“å­˜å ç”¨éå¸¸å¤§ã€‚\nåœ¨ MHA ä¸­ï¼Œæ¯ä¸ª head çš„ QKV çš„ç»´åº¦æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯ d<sub>h</sub>ï¼Œå› æ­¤ï¼Œä¸ºäº†å‹ç¼© KV å€¼ï¼Œç½—ç¦ç†æå‡ºäº† MLA çš„ç»“æ„ï¼Œå°† KV å€¼è¿›è¡Œå‹ç¼©ï¼Œé™ä½äº†æ˜¾å­˜å ç”¨ã€‚\n\n**11. è¯·æ ¹æ®MLAçš„ç»“æ„ç»™å‡ºpytorchä»£ç ã€‚**\næŠ±æ­‰ï¼Œæˆ‘æ— æ³•æä¾›è§†é¢‘ä¸­æåˆ°çš„ä»£ç ï¼Œå› ä¸ºæˆ‘æ²¡æ³•ä½¿ç”¨ä»£ç ã€‚\n\n**12. MoEä¸­è®¡ç®—å¾—åˆ†æ—¶ï¼Œb_içš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿåœ¨å·¥ç¨‹å®ç°æ—¶æ˜¯æœ‰ä»€ä¹ˆæ³¨æ„çš„ç»†èŠ‚ï¼Ÿ**\nb<sub>i</sub> ç”¨æ¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªä¸“å®¶çš„ batch sizeï¼Œä»è€Œè¾¾åˆ°è´Ÿè½½å‡è¡¡çš„ç›®çš„ã€‚b<sub>i</sub> ä¸å‚ä¸å‰é¦ˆå…¨è¿æ¥å±‚çš„è®¡ç®—ã€‚\n\n**13. è¯·ä»‹ç»MoEä¸­çš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚è§†é¢‘ä¸­æ‰€ç»™å‡ºçš„è‹¥å¹²å…¬å¼åˆ†åˆ«åœ¨è®¡ç®—ä»€ä¹ˆï¼Ÿ**\nMoE è®­ç»ƒé‡‡ç”¨è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚æ¯ä¸ª token éƒ½ä¼šæ¿€æ´» Top K ä¸ªä¸“å®¶ï¼Œå…¶ä¸­ K=4ï¼Œæ¿€æ´»ä¸“å®¶æ€»æ•°ä¸º N<sub>E</sub>=64ã€‚è§†é¢‘ä¸­ç»™å‡ºçš„è‹¥å¹²å…¬å¼åˆ†åˆ«ç”¨äºè®¡ç®—ï¼š\n* f<sub>i</sub>ï¼šä¸“å®¶ i è¢«æ¿€æ´»çš„æ¬¡æ•°ï¼›\n* P<sub>i</sub>ï¼šä¸“å®¶ i è¢«æ¿€æ´»çš„æ¦‚ç‡ï¼›\n* L<sub>Bal</sub>ï¼šè´Ÿè½½å‡è¡¡æŸå¤±å‡½æ•°ã€‚\n\n**14. ä½¿ç”¨FP8ä»£æ›¿FP32å’ŒFP16è®­ç»ƒæœ‰ä»€ä¹ˆä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚ä¸ºäº†è§£å†³FP8è®­ç»ƒçš„å›°éš¾ï¼ŒDeepseekV3æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ**\nFP8 è®­ç»ƒçš„ç‰¹ç‚¹ï¼š\nä¼˜ç‚¹ï¼š1. è®¡ç®—é€Ÿåº¦å¿«ï¼›2. å†…å­˜å ç”¨æ›´å°‘ã€‚\nç¼ºç‚¹ï¼š3. æ¿€æ´»æ¢¯åº¦è¡°å‡ï¼›4. æ•°å€¼èŒƒå›´å—é™ã€‚\nä¸ºäº†è§£å†³ FP8 è®­ç»ƒçš„å›°éš¾ï¼ŒDeepseek V3 å¯¹ FP32 æ•°å€¼åšäº†ç¼©æ”¾ï¼Œå‹ç¼©æ•°å€¼èŒƒå›´è‡³[0, 1]ï¼Œç„¶åå°†å‹ç¼©åçš„æ•°å€¼åˆ†æˆå¤šä¸ªå°å—ï¼Œå¯¹æ¯ä¸ªå°å—åšé‡åŒ–ï¼Œæœ€åç»„åˆæˆä¸€ä¸ªå¼ é‡ä½œä¸ºç»“æœã€‚\n\n**15. DeepseekV3çš„FP8è®­ç»ƒä¸­ï¼Œå‰å‘è¿ç®—è¿‡ç¨‹ä¸­å…·ä½“æ˜¯ç”¨ä»€ä¹ˆæ•°æ®æ ¼å¼ä¿å­˜å“ªä¸€æ­¥çš„å˜é‡ï¼Ÿ**\nå‰å‘è¿ç®—è¿‡ç¨‹ï¼šè¾“å…¥æ˜¯ BF16ï¼Œæƒé‡æ˜¯ FP8ã€‚è®¡ç®—è¿‡ç¨‹å…ˆå°†è¾“å…¥å’Œæƒé‡è½¬æ¢æˆ FP32ï¼Œç„¶åè¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚çŸ©é˜µä¹˜æ³•è¿ç®—è¿‡ç¨‹æ˜¯åœ¨ Tensor Core ä¸­ç”¨ FP8 è¿›è¡Œè®¡ç®—çš„ï¼Œç»“æœç”¨ FP32 è¿›è¡Œä¿å­˜ã€‚\n\n**16. æŠŠä¸€ä¸ªFP32çš„æ•°è½¬æ¢æˆFP8éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ**\nä¸€ä¸ª FP32 æ•°è½¬æ¢æˆ FP8 éœ€è¦ä¸¤æ­¥ï¼š\n1. Unscaled FP32 = FP32 / scale\n2. FP8 = Convert(Unscaled FP32)\nå…¶ä¸­ scale æ˜¯æ ¹æ®tensorçš„æ•°å€¼èŒƒå›´å’Œç²¾åº¦å¾—å‡ºçš„ã€‚\n\n**17. è¦è®¡ç®—å°†FP32çš„æ•°è½¬æ¢æˆFP8æ‰€éœ€scaleçš„å¹…åº¦ï¼Œæœ‰å“ªå‡ ç§é‡åŒ–æ–¹å¼ï¼Ÿ**\næœ‰å››ç§é‡åŒ–æ–¹å¼ï¼š\n1. per tensorï¼ˆæ•´ä¸ªå¼ é‡é‡‡ç”¨åŒä¸€ä¸ªscaleå€¼ï¼‰ï¼›\n2. per tokenï¼ˆä¸€ä¸ª token å¯¹åº”ä¸€ä¸ª scale å€¼ï¼‰ï¼›\n3. group wiseï¼ˆæ¯ä¸ªåˆ†ç»„é‡‡ç”¨ä¸€ä¸ª scale å€¼ï¼‰ï¼›\n4. tile wiseï¼ˆæ¯ä¸ªç“¦ç‰‡é‡‡ç”¨ä¸€ä¸ª scale å€¼ï¼‰ã€‚\nDeepseek V3 é‡‡ç”¨äº† group wise æ–¹å¼ï¼Œå¹¶ä½¿ç”¨ tile wise è¿›è¡Œé‡åŒ–ã€‚\n\n**18. è¯·è¯¦ç»†è§£é‡ŠFP8æ–¹æ¡ˆæ˜¯æ€ä¹ˆåšçŸ©é˜µä¹˜æ³•çš„ï¼Ÿä½ å¯ä»¥ä¸¾ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚**\nå¯¹äºè¾“å…¥å¼ é‡å’Œæƒé‡å¼ é‡ï¼Œå°†å®ƒä»¬åˆ†æˆè‹¥å¹²ä¸ªå°å—ã€‚å°†æ¯ä¸ªå°å—è½¬æ¢ä¸º FP8ï¼Œå¹¶åœ¨ Tensor Core ä¸­åšçŸ©é˜µä¹˜æ³•ï¼Œæœ€åå°†ç»“æœç”¨ FP32 ä¿å­˜ã€‚åœ¨ NVIDIA H800 GPU ä¸Šï¼ŒFP8 GEMM ä¸ºçŸ©é˜µè¿ç®—æä¾›äº† 32x8 çš„é€Ÿåº¦å¢ç›Šã€‚\n\n**19. FP8çš„E4M3å’ŒE5M2æ ¼å¼çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**\nE4M3 æ ¼å¼ï¼š4 ä½æŒ‡æ•°ã€3 ä½å°¾æ•°ã€1 ä½ç¬¦å·ä½ï¼›\nE5M2 æ ¼å¼ï¼š5 ä½æŒ‡æ•°ã€2 ä½å°¾æ•°ã€1 ä½ç¬¦å·ä½ã€‚\nDeepseek V3 é‡‡ç”¨äº† E5m2 æ··åˆç²¾åº¦æ¨¡å‹ï¼Œåªå¯¹å°‘æ•°æœ€å°çš„æƒé‡ä½¿ç”¨ FP32ã€‚\n\n**20. DeepseekV3å¯¹äºä¸­å›½çš„å¤§æ¨¡å‹è¡Œä¸šå¸¦æ¥äº†ä»€ä¹ˆå½±å“ï¼Ÿ**\nDeepseekV3 ä½œä¸ºé¦–ä¸ªå¼€æºçš„ FP8 æ··åˆç²¾åº¦æ¨¡å‹ï¼Œä¸ºå¤§æ¨¡å‹çš„å‘å±•å¸¦æ¥äº†æ–°çš„æ€è·¯ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œä¹Ÿä¸ºèµ„æºæœ‰é™çš„ä¸ªäººæˆ–å°å…¬å¸æä¾›äº†æ¢ç´¢å¤§æ¨¡å‹çš„å¯èƒ½æ€§ã€‚\n\nå¸Œæœ›è¿™äº›ç­”æ¡ˆå¯¹ä½ æœ‰å¸®åŠ©ã€‚"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "# Create the prompt.\n",
        "prompt = \"\"\"\n",
        "è§’è‰²ï¼šä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰ç”¨çš„è§†é¢‘åŠ©æ‰‹ï¼Œå¯ä»¥è§‚çœ‹è§†é¢‘å†…å®¹å¹¶å›ç­”å…³äºç›¸å…³é—®é¢˜ã€‚\n",
        "ä»»åŠ¡ï¼šè¯·ä½ è§‚çœ‹ä¸‹é¢è¿™æ®µè§†é¢‘ï¼Œä»…ä»…åŸºäºè§†é¢‘å†…å®¹å›ç­”é—®é¢˜åˆ—è¡¨ä¸­çš„é—®é¢˜ï¼Œå›ç­”æ¸…æ™°å‡†ç¡®ã€‚å›ç­”æ ¼å¼æ˜¯å…ˆåˆ—å‡ºé—®é¢˜ï¼Œå†ç»™å‡ºå›ç­”ã€‚\n",
        "é—®é¢˜åˆ—è¡¨\n",
        "1.\tDeepseekV3çš„è®­ç»ƒç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "2.\tè¯·åˆ—å‡ºDeepseekV3çš„ä¸»è¦åˆ›æ–°ç‚¹\n",
        "3.\tåœ¨DeepseekV3åšæ¨ç†æ—¶ï¼Œå®é™…ä¸Šæ¯ä¸ªtokenåªæœ‰å¤šå°‘å‚æ•°è¢«æ¿€æ´»ï¼Ÿ\n",
        "4.\tDeepSeek-V3ä½¿ç”¨äº†å¤šå¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Ÿ\n",
        "5.\tå¤§æ¨¡å‹æ¨ç†æ—¶å€™å“ªä¸¤ä¸ªæŒ‡æ ‡å¾ˆé‡è¦ï¼Ÿå®ƒä»¬åˆ†åˆ«è¡¡é‡äº†ä»€ä¹ˆï¼Ÿ\n",
        "6.\tDeepseekV3çš„æ¨ç†é˜¶æ®µä½¿ç”¨äº†ä»€ä¹ˆæ¶æ„ï¼Ÿè¯·ä»‹ç»è¿™ä¸ªæ¶æ„çš„ç‰¹ç‚¹å’Œç­–ç•¥ã€‚\n",
        "7.\tDeepseekV3åœ¨Prefillé˜¶æ®µçš„Attentionéƒ¨åˆ†ä¸ºä»€ä¹ˆè¦è®¾ç½®è¾ƒå°çš„TPæ•°é‡4ï¼Ÿ\n",
        "8.\tè¯·ä»‹ç»Prefillé˜¶æ®µæ‰€ä½¿ç”¨çš„å†—ä½™ä¸“å®¶çš„æ¦‚å¿µã€‚\n",
        "9.\tè¯·è§£é‡Šdevice-limited routingæ˜¯æ€ä¹ˆè¿ä½œçš„ï¼Ÿæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ\n",
        "10.\tMLAåœ¨MHAä¸Šåšäº†ä»€ä¹ˆæ ·çš„æ”¹è¿›ï¼Ÿæ˜¯ä¸ºäº†è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n",
        "11.\tè¯·æ ¹æ®MLAçš„ç»“æ„ç»™å‡ºpytorchä»£ç ã€‚\n",
        "12.\tMoEä¸­è®¡ç®—å¾—åˆ†æ—¶ï¼Œb_içš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿåœ¨å·¥ç¨‹å®ç°æ—¶æ˜¯æœ‰ä»€ä¹ˆæ³¨æ„çš„ç»†èŠ‚ï¼Ÿ\n",
        "13.\tè¯·ä»‹ç»MoEä¸­çš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚è§†é¢‘ä¸­æ‰€ç»™å‡ºçš„è‹¥å¹²å…¬å¼åˆ†åˆ«åœ¨è®¡ç®—ä»€ä¹ˆï¼Ÿ\n",
        "14.\tä½¿ç”¨FP8ä»£æ›¿FP32å’ŒFP16è®­ç»ƒæœ‰ä»€ä¹ˆä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚ä¸ºäº†è§£å†³FP8è®­ç»ƒçš„å›°éš¾ï¼ŒDeepseekV3æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ\n",
        "15.\tDeepseekV3çš„FP8è®­ç»ƒä¸­ï¼Œå‰å‘è¿ç®—è¿‡ç¨‹ä¸­å…·ä½“æ˜¯ç”¨ä»€ä¹ˆæ•°æ®æ ¼å¼ä¿å­˜å“ªä¸€æ­¥çš„å˜é‡ï¼Ÿ\n",
        "16.\tæŠŠä¸€ä¸ªFP32çš„æ•°è½¬æ¢æˆFP8éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ\n",
        "17.\tè¦è®¡ç®—å°†FP32çš„æ•°è½¬æ¢æˆFP8æ‰€éœ€scaleçš„å¹…åº¦ï¼Œæœ‰å“ªå‡ ç§é‡åŒ–æ–¹å¼ï¼Ÿ\n",
        "18.\tè¯·è¯¦ç»†è§£é‡ŠFP8æ–¹æ¡ˆæ˜¯æ€ä¹ˆåšçŸ©é˜µä¹˜æ³•çš„ï¼Ÿä½ å¯ä»¥ä¸¾ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚\n",
        "19.\tFP8çš„E4M3å’ŒE5M2æ ¼å¼çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "20.\tDeepseekV3å¯¹äºä¸­å›½çš„å¤§æ¨¡å‹è¡Œä¸šå¸¦æ¥äº†ä»€ä¹ˆå½±å“ï¼Ÿ\n",
        "\"\"\"\n",
        "contents = [video_file, prompt]\n",
        "\n",
        "print(\"Making LLM inference request...\")\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=stream,\n",
        ")\n",
        "print(\"Successfully generating reponse:\")\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9d345e9868"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Docs on ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-gemini/cookbook/blob/main/quickstarts\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />More notebooks in the Cookbook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91AeeGO1ncU"
      },
      "source": [
        "## [optional] Show the conversation\n",
        "\n",
        "This section displays the conversation received from Google AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yoL3p3KPylFW"
      },
      "outputs": [],
      "source": [
        "# @title Show the conversation, in colab.\n",
        "import mimetypes\n",
        "\n",
        "def show_file(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "\n",
        "    if drive_id := file_data.get(\"drive_id\", None):\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        name = path\n",
        "        # data = path.read_bytes()\n",
        "        kwargs = {\"filename\": path}\n",
        "    elif url := file_data.get(\"url\", None):\n",
        "        name = url\n",
        "        kwargs = {\"url\": url}\n",
        "        # response = requests.get(url)\n",
        "        # data = response.content\n",
        "    elif data := file_data.get(\"inline_data\", None):\n",
        "        name = None\n",
        "        kwargs = {\"data\": data}\n",
        "    elif name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files to \"\n",
        "                'Colab using the file manager (\"ğŸ“ Files\"in the left toolbar)'\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "        print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "        return\n",
        "\n",
        "    format = mimetypes.guess_extension(mime_type).strip(\".\")\n",
        "    if mime_type.startswith(\"image/\"):\n",
        "        image = IPython.display.Image(**kwargs, width=256)\n",
        "        IPython.display.display(image)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if mime_type.startswith(\"audio/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Audio(**kwargs)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    if mime_type.startswith(\"video/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Video(**kwargs, mimetype=mime_type)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "\n",
        "\n",
        "for content in gais_contents:\n",
        "    if role := content.get(\"role\", None):\n",
        "        print(\"Role:\", role, \"\\n\")\n",
        "\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if text := part.get(\"text\", None):\n",
        "            print(text, \"\\n\")\n",
        "\n",
        "        elif file_data := part.get(\"file_data\", None):\n",
        "            show_file(file_data)\n",
        "\n",
        "    print(\"-\" * 80, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}