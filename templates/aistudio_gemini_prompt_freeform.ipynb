{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "kWIuwKG2_oWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d95014-96f3-44c2-aecf-517cd0c7810d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "    # The SDK will automatically read it from the GOOGLE_API_KEY environment variable.\n",
        "    # In Colab get the key from Colab-secrets (\"ğŸ”‘\" in the left panel).\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Parse the arguments\n",
        "\n",
        "model = 'gemini-1.5-pro' # @param {isTemplate: true}\n",
        "contents_b64 = 'W10=' # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MSwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9' # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False\n",
        "\n",
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"ğŸ“ Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocd6brCaYgCM",
        "outputId": "9697c08e-1c58-4dab-fe19-6f02dcd80439"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the video and print a confirmation.\n",
        "video_file_name = \"/content/drive/MyDrive/å¾®è§‚ç»æµå­¦/MITå¾®è§‚ç»æµå­¦ProductionTHeory.mp4\"\n",
        "\n",
        "print(f\"Uploading file...\")\n",
        "video_file = genai.upload_file(path=video_file_name)\n",
        "print(f\"Completed upload: {video_file.uri}\")\n",
        "import time\n",
        "\n",
        "# Check whether the file is ready to be used.\n",
        "while video_file.state.name == \"PROCESSING\":\n",
        "    print('.', end='')\n",
        "    time.sleep(10)\n",
        "    video_file = genai.get_file(video_file.name)\n",
        "\n",
        "if video_file.state.name == \"FAILED\":\n",
        "  raise ValueError(video_file.state.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCEHJ-l8Yk-3",
        "outputId": "b5dee2bd-c19e-40b4-9f81-316992b9282f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading file...\n",
            "Completed upload: https://generativelanguage.googleapis.com/v1beta/files/u6ea2vjh3612\n",
            ".........."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "# Create the prompt.\n",
        "prompt = \"\"\"è§’è‰²è®¾å®š: ä½ æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ­£åœ¨å­¦ä¹ åˆ‘æ³•è¯¾ï¼Œæƒ³è¦åšåˆ°èƒ½å¤Ÿæ¸…æ™°ã€å‡†ç¡®åœ°ç†è§£æ³•å¾‹æ–‡æœ¬ï¼Œå¹¶èƒ½ç»“åˆå®é™…æ¡ˆä¾‹è¿›è¡Œåˆ†æã€‚\n",
        "ä»»åŠ¡æè¿°: è¯·è§‚çœ‹æˆ‘ä¸‹é¢ç»™å‡ºçš„åˆ‘æ³•æ€»è®ºçš„å…³äºè´¿èµ‚çŠ¯ç½ªçš„è¯¾ç¨‹è§†é¢‘ã€‚åœ¨è§‚çœ‹å®Œè§†é¢‘åï¼Œè¯·å›ç­”ä¸‹åˆ—é—®é¢˜ã€‚ç¡®ä¿ä½ çš„å›ç­”è¯¦ç»†ã€å‡†ç¡®ï¼Œå¹¶ç»“åˆè§†é¢‘ä¸­çš„ç›¸å…³ä¿¡æ¯æ¥æ”¯æŒä½ çš„ç­”æ¡ˆã€‚\n",
        "\"\"\"\n",
        "contents = [prompt]\n",
        "\n",
        "print(\"Making LLM inference request...\")\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=stream,\n",
        ")\n",
        "print(\"Successfully generating reponse:\")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "6TD1HaXTLkAw",
        "outputId": "baa168da-645f-46ef-ad32-8a82853d4e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making LLM inference request...\n",
            "Successfully generating reponse:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "è¯·æä¾›è¯¾ç¨‹è§†é¢‘ã€‚æˆ‘æ²¡æœ‰è§‚çœ‹è§†é¢‘çš„åŠŸèƒ½ï¼Œæ— æ³•æ ¹æ®è§†é¢‘å†…å®¹å›ç­”ä½ çš„é—®é¢˜ã€‚è¯·ä½ æä¾›è§†é¢‘çš„æ–‡å­—ç¨¿æˆ–è€…æ€»ç»“è§†é¢‘ä¸­å…³äºè´¿èµ‚ç½ªçš„å…³é”®ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š\n\n* è´¿èµ‚ç½ªçš„å®šä¹‰å’Œæ„æˆè¦ä»¶\n* ä¸åŒç±»å‹çš„è´¿èµ‚ç½ªï¼ˆä¾‹å¦‚ï¼šå—è´¿ç½ªã€è¡Œè´¿ç½ªã€å•ä½è¡Œè´¿ç½ªç­‰ç­‰ï¼‰åŠå…¶åŒºåˆ«\n* è´¿èµ‚ç½ªçš„è®¤å®šæ ‡å‡†ï¼Œä¾‹å¦‚â€œè´¢ç‰©â€çš„èŒƒå›´ã€â€œèŒåŠ¡è¡Œä¸ºâ€çš„è§£é‡Š\n* ç›¸å…³æ¡ˆä¾‹åˆ†æï¼ŒåŒ…æ‹¬æ¡ˆæƒ…æè¿°ã€æ³•é™¢åˆ¤å†³ä»¥åŠåˆ¤å†³ç†ç”±\n* å…è´£äº‹ç”±å’Œä»è½»ã€å‡è½»å¤„ç½šæƒ…èŠ‚\n\nä½ æä¾›çš„ä¿¡æ¯è¶Šè¯¦ç»†ï¼Œæˆ‘å°±èƒ½æ›´å¥½åœ°å¸®åŠ©ä½ ç†è§£è´¿èµ‚ç½ªçš„ç›¸å…³çŸ¥è¯†ï¼Œå¹¶è¿›è¡Œæ¡ˆä¾‹åˆ†æã€‚\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zAD69vE92b"
      },
      "source": [
        "## Call `generate_content`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LB2LxPmAB95V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b46c17a2-6ae3-42bf-86b9-71404c65612b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making LLM inference request...\n",
            "Successfully generating reponse:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, here are the answers to your questions, based on the MIT OpenCourseWare video \"Lecture 5: Production Theory\" for the course \"Principles of Microeconomics (14.01),\" Fall 2018.\n\n**Knowledge Questions (10):**\n\n1. **What is a production function?**\nA production function describes the relationship between the inputs a firm uses and the quantity of output it produces. It shows the maximum amount of output that can be produced from a given set of inputs, given the available technology.  Mathematically, it can be represented as q = f(L,K), where q is the quantity of output, L is labor, and K is capital.\n\n2. **What is the firm's primary objective in producer theory?**\nThe firm's primary objective in producer theory is to maximize profits (Î ), which is defined as the difference between total revenue (R) and total cost (C):  Max Î  = R - C.\n\n3. **Define \"factors of production.\"**\nFactors of production are the inputs used by firms to produce goods and services.  The video focuses on two primary factors: labor (L) and capital (K).  Other factors, such as land and raw materials, are sometimes considered as well.\n\n4. **Distinguish between variable and fixed inputs.**\nVariable inputs are those that can be changed in the short run, such as the number of workers employed.  Fixed inputs are those that cannot be changed in the short run, like the size of a factory or the number of machines.\n\n5. **How are the short run and long run defined in producer theory?**\nThe short run is the period of time during which at least one input is fixed. The long run is the period of time during which all inputs are variable.\n\n6. **What is an isoquant?**\nAn isoquant is a curve that shows all the different combinations of inputs (L and K) that can produce the same level of output (q).  It's like an indifference curve for producers.\n\n7. **What does the marginal rate of technical substitution (MRTS) represent?**\nThe MRTS represents the rate at which a firm can substitute one input for another (e.g., capital for labor) while holding output constant.  It is the slope of the isoquant.\n\n8. **What is the mathematical relationship between MRTS and marginal products of labor (MPL) and capital (MPK)?**\nMRTS = -Î”K/Î”L = MPL/MPK.  This means the MRTS is equal to the ratio of the marginal product of labor to the marginal product of capital.\n\n9. **What does the term \"returns to scale\" refer to?**\nReturns to scale describes how output changes when all inputs are increased proportionately.  There are three types: increasing returns to scale (output increases more than proportionally), constant returns to scale (output increases proportionally), and decreasing returns to scale (output increases less than proportionally).\n\n10. **Give an example of an industry with potentially decreasing returns to scale.**\nThe video does not offer a specific example, though the speaker mentions industries that are dependent on fixed natural resources might exhibit this. The speaker presents an example with tobacco where output increases less than proportionally to increases in the inputs.\n\n\n\n**Application and Analysis Questions (10):**\n\n11. **A software company uses programmers (L) and computers (K) to produce lines of code (q) according to the production function q = 10L^0.5 K^0.5. Does this production function exhibit increasing, decreasing, or constant returns to scale? Explain your answer.**\nThis production function exhibits constant returns to scale.  If you double both L and K, output doubles. For example: q = 10(2L)^0.5(2K)^0.5 = 10*2^0.5*L^0.5*2^0.5*K^0.5 = 2*10L^0.5K^0.5 = 2q.\n\n12. **Suppose a firmâ€™s production function is q = L + K. Draw the isoquants for q = 10, q = 20, and q = 30. What does the shape of these isoquants tell you about the relationship between labor and capital in this production process?**\nThe isoquants would be straight lines with a slope of -1.  This indicates that labor and capital are perfectly substitutable in this production process; one unit of labor can always be replaced by one unit of capital, and vice versa.\n\n13. **A furniture maker experiences increasing returns to scale initially, followed by decreasing returns to scale as output grows. Explain why this might occur, relating your answer to specialization and coordination costs.**\nInitially, increasing returns might occur due to specialization.  As the firm grows, workers can specialize in specific tasks, leading to higher productivity. However, as the firm gets much larger, coordination costs can increase.  Managing a large number of specialized workers can become complex and inefficient, leading to decreasing returns.\n\n14. **Consider a Leontief production function for making bicycles, where wheels (W) and frames (F) are the inputs: q = min(W, F). What are the implications of this production function for a bicycle manufacturer's input choices?**\nThis production function implies that wheels and frames are perfect complements.  The manufacturer needs exactly one wheel and one frame for each bicycle.  Having more of one input without a corresponding increase in the other will not increase output. They must be used in fixed proportions.\n\n15. **The lecture discusses the historical trend of productivity growth in the US. If productivity growth remains low, what are the implications for future increases in the standard of living? What policies could potentially stimulate productivity growth?**\nLow productivity growth leads to slower growth in the standard of living, meaning wages and overall economic well-being will improve more slowly over time.  Policies to stimulate productivity growth include investment in education, research and development, infrastructure, and policies that promote competition and innovation.\n\n16. **Discuss the trade-off between increased consumption and increased leisure that societies face as productivity grows. Use examples to illustrate how different countries have made different choices in this regard.**\nAs productivity grows, societies can choose to produce more goods and services (increased consumption) or to work less and enjoy more leisure time.  The US, for example, has generally favored increased consumption over increased leisure compared to many European countries, which often have higher taxes, stronger social safety nets, and more vacation time.\n\n17. **Why is innovation crucial for long-run economic growth, particularly given the concept of diminishing marginal product?**\nDiminishing marginal product implies that adding more of the same inputs (labor and capital) will eventually lead to smaller and smaller increases in output.  Innovation, by improving technology and creating new products and processes (represented by the â€œAâ€ term in the production function), is crucial for shifting the production function upward and overcoming diminishing marginal product, leading to sustained economic growth.\n\n18. **The lecture highlights that the benefits of productivity growth have not been equally distributed since the 1970s. What are some potential explanations for this unequal distribution?**\nSome explanations include skill-biased technological change (technology favors highly skilled workers), globalization and increased competition (lowering wages for some), declining unionization (reducing bargaining power), and changes in government policies (tax cuts for the wealthy).\n\n19. **Explain why Malthus' prediction of mass starvation due to limited land availability did not come to pass.**\nMalthus failed to anticipate the impact of technological innovation in agriculture.  Innovations like fertilizers, pesticides, and genetically modified crops dramatically increased agricultural productivity, allowing food production to keep pace with population growth.\n\n20. **Imagine a firm with the production function q = 2L + K. If the wage rate (w) is $10 and the rental rate of capital (r) is $5, what combination of labor and capital should the firm use to minimize the cost of producing 100 units of output?**\nSince capital is cheaper per unit of output produced, the firm should use only capital to minimize costs. It will use K = 100 and L = 0.\n\n\n\nLet me know if you have any other questions."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "# Create the prompt.\n",
        "prompt = \"\"\"\n",
        "è§’è‰²è®¾å®š: ä½ æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ­£åœ¨å­¦ä¹ åˆ‘æ³•è¯¾ï¼Œæƒ³è¦åšåˆ°èƒ½å¤Ÿæ¸…æ™°ã€å‡†ç¡®åœ°ç†è§£æ³•å¾‹æ–‡æœ¬ï¼Œå¹¶èƒ½ç»“åˆå®é™…æ¡ˆä¾‹è¿›è¡Œåˆ†æã€‚\n",
        "ä»»åŠ¡æè¿°: è¯·è§‚çœ‹æˆ‘ä¸‹é¢ç»™å‡ºçš„åˆ‘æ³•æ€»è®ºçš„å…³äºè´¿èµ‚çŠ¯ç½ªçš„è¯¾ç¨‹è§†é¢‘ã€‚åœ¨è§‚çœ‹å®Œè§†é¢‘åï¼Œè¯·å›ç­”ä¸‹åˆ—é—®é¢˜ã€‚ç¡®ä¿ä½ çš„å›ç­”è¯¦ç»†ã€å‡†ç¡®ï¼Œå¹¶ç»“åˆè§†é¢‘ä¸­çš„ç›¸å…³ä¿¡æ¯æ¥æ”¯æŒä½ çš„ç­”æ¡ˆã€‚\n",
        "é—®é¢˜åˆ—è¡¨:\n",
        "1.\tå—è´¿ç½ªçš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "2.\tå—è´¿ç½ªçš„æ—¢é‚æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "3.\tæ€§è´¿èµ‚åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æ„æˆå—è´¿ç½ªï¼Ÿ\n",
        "4.\tå›½å®¶å·¥ä½œäººå‘˜æ”¶å—è´¢ç‰©åï¼Œä»€ä¹ˆæƒ…å†µä¸‹ä¸æ„æˆå—è´¿ç½ªï¼Ÿ\n",
        "5.\tå—è´¿ç½ªçš„äº”ç§è¡Œä¸ºæ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "6.\tç´¢è´¿å’Œè¯ˆéª—ç½ªçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "7.\tå•çº¯å—è´¿çš„æˆç«‹æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "8.\tæ–¡æ—‹å—è´¿çš„æ„æˆè¦ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "9.\täº‹åå—è´¿å¦‚ä½•è®¤å®šï¼Ÿ\n",
        "10.\tè¡Œè´¿ç½ªçš„æ„æˆè¦ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "11.\tæŸåŒ»é™¢é™¢é•¿æ”¶å—åŒ»è¯ä»£è¡¨æä¾›çš„æ—…æ¸¸æœºä¼šï¼Œä»·å€¼10ä¸‡å…ƒï¼Œä½†é™¢é•¿å¹¶æœªä¸ºè¯¥åŒ»è¯ä»£è¡¨è°‹å–ä»»ä½•åˆ©ç›Šã€‚é™¢é•¿æ˜¯å¦æ„æˆå—è´¿ç½ªï¼Ÿ\n",
        "12.\tæŸå…¬å¸ä¸ºè·å¾—æ”¿åºœé¡¹ç›®ï¼Œå‘ä¸»ç®¡å®˜å‘˜èµ é€äº†ä¸€å¹…åç”»ï¼Œä»·å€¼50ä¸‡å…ƒã€‚äº‹åï¼Œè¯¥å…¬å¸è·å¾—äº†è¯¥é¡¹ç›®ã€‚è¿™å¹…åç”»å¦‚ä½•è®¤å®šï¼Ÿ\n",
        "13.\tæŸå®˜å‘˜çš„å¦»å­æ”¶å—äº†å•†äºº20ä¸‡å…ƒç°é‡‘ï¼Œå¹¶æ‰¿è¯ºè®©å…¶ä¸ˆå¤«åœ¨é¡¹ç›®å®¡æ‰¹ä¸Šç»™äºˆå…³ç…§ã€‚è¯¥å®˜å‘˜å¯¹å¦»å­æ”¶é’±çš„äº‹æƒ…å¹¶ä¸çŸ¥æƒ…ã€‚å¦»å­å’Œå•†äººåˆ†åˆ«æ„æˆä»€ä¹ˆç½ªï¼Ÿ\n",
        "14.\tæŸå®˜å‘˜åœ¨é€€ä¼‘åæ”¶å—äº†ä¼ä¸š50ä¸‡å…ƒï¼Œæ„Ÿè°¢å…¶åœ¨èŒæœŸé—´çš„å…³ç…§ã€‚è¿™æ„æˆäº‹åå—è´¿å—ï¼Ÿå¦‚ä½•è®¤å®šï¼Ÿ\n",
        "15.\tæŸä¼ä¸šåœ¨ä¸å›½ä¼è¿›è¡Œç»æµå¾€æ¥æ—¶ï¼ŒæŒ‰ç…§è¡Œä¸šæƒ¯ä¾‹ï¼Œç»™äºˆå›½ä¼è´Ÿè´£äººä¸€å®šçš„å›æ‰£ã€‚è¿™æ˜¯å¦æ„æˆè¡Œè´¿ç½ªï¼Ÿ\n",
        "16.\tæŸå…¬å¸ä¸ºäº†è·å¾—ä¸€é¡¹å·¥ç¨‹ï¼Œå‘å®˜å‘˜è¡Œè´¿10ä¸‡å…ƒï¼Œä½†è¯¥å®˜å‘˜å¹¶æœªæ‰¿è¯ºä»»ä½•äº‹æƒ…ï¼Œä¹Ÿæœªå®é™…ä¸ºè¯¥å…¬å¸è°‹å–åˆ©ç›Šã€‚è¿™æ„æˆè¡Œè´¿ç½ªå—ï¼Ÿ\n",
        "17.\tææŸä¸ºå¸®åŠ©æœ‹å‹è·å¾—é“¶è¡Œè´·æ¬¾ï¼Œå‘é“¶è¡Œå·¥ä½œäººå‘˜è¡Œè´¿5ä¸‡å…ƒã€‚ä½†ç”±äºè¯¥æœ‹å‹çš„èµ„è´¨ä¸ç¬¦åˆè´·æ¬¾æ¡ä»¶ï¼Œæœ€ç»ˆæœªèƒ½è·å¾—è´·æ¬¾ã€‚ææŸçš„è¡Œä¸ºå¦‚ä½•è®¤å®šï¼Ÿ\n",
        "18.\tæŸé¢†å¯¼çš„å¸æœºç»å¸¸æ”¶å—ä¸€äº›ä¼ä¸šé€çš„è´­ç‰©å¡å’ŒåœŸç‰¹äº§ï¼Œä»·å€¼æ•°åƒå…ƒï¼Œå¹¶å‘ŠçŸ¥é¢†å¯¼æ˜¯è°é€çš„ã€‚é¢†å¯¼æœªè¡¨ç¤ºåå¯¹ï¼Œä¹Ÿæœªè®©å¸æœºé€€è¿˜ã€‚é¢†å¯¼æ˜¯å¦æ„æˆå—è´¿ç½ªï¼Ÿ\n",
        "19.\tå¼ æŸçš„å„¿å­æƒ³å‚å†›ï¼Œå¼ æŸå‘è´Ÿè´£å¾å…µçš„å®˜å‘˜é€äº†2ä¸‡å…ƒã€‚ä½†å…¶å„¿å­å› èº«ä½“åŸå› æœªèƒ½é€šè¿‡ä½“æ£€ã€‚å¼ æŸçš„è¡Œä¸ºå¦‚ä½•è®¤å®šï¼Ÿ\n",
        "20.\tæŸå®˜å‘˜çš„ç§˜ä¹¦åˆ©ç”¨å®˜å‘˜çš„å½±å“åŠ›ï¼Œæ”¶å—ä¼ä¸š30ä¸‡å…ƒï¼Œæ‰¿è¯ºä¸ºä¼ä¸šåœ¨é¡¹ç›®å®¡æ‰¹ä¸Šæä¾›å¸®åŠ©ã€‚å®˜å‘˜å¯¹ç§˜ä¹¦çš„è¡Œä¸ºå¹¶ä¸çŸ¥æƒ…ã€‚ç§˜ä¹¦çš„è¡Œä¸ºå¦‚ä½•è®¤å®šï¼Ÿ\n",
        "21.\tä¸™ä¸ºæŸç¨åŠ¡å±€å±€é•¿ï¼Œåˆ˜æŸå› ä¼ä¸šæ¼ç¨è¢«æŸ¥ï¼Œæ‰¾ä¸™ç–é€šå…³ç³»ï¼Œå¹¶äº¤ç»™ä¸™2ä¸‡å…ƒç°é‡‘ï¼Œè¢«ä¸™å©‰æ‹’ã€‚ä¸¤äººå¯’æš„ä¸€äºŒï¼Œä¸™é—®åˆ˜æŸæ˜¯å¦èƒ½å¸®å…¶å®‰æ’æƒ…å¦‡ç‹å¥³åˆ°ä¼ä¸šå·¥ä½œï¼Œåˆ˜æŸç­”åº”ã€‚æ¬¡æ—¥åˆ˜æŸè˜ä»»å¹¶æ— ä¼šè®¡èµ„æ ¼çš„ç‹å¥³ä¸ºä¼ä¸šè´¢åŠ¡å‰¯æ€»ç›‘ï¼Œç‹å¥³ä»æœªä¸Šç­ï¼Œä½†åˆ˜æŸæŒ‰æœˆæ”¯ä»˜ä¸‡å…ƒå·¥èµ„ã€‚ä¸™æ„æˆå—è´¿ç½ªå—ï¼Ÿ\n",
        "22.\tå¼ ä¸‰å¯¹æ±‚ä»–åœ¨åŠå…¬æ¥¼ç§Ÿå”®ä¸­â€œè¡Œæ–¹ä¾¿â€çš„å´æŸè°ç§°ï¼Œè‡ªå·±çš„å¤–ç”¥å‡†å¤‡å‡ºå›½ç•™å­¦ã€‚å´æŸäºæ˜¯å°†20ä¸‡å…ƒæ‰“å…¥å¼ ä¸‰æŒ‡å®šçš„é“¶è¡Œå¡è´¦æˆ·ã€‚å¼ ä¸‰è¯¥å½“ä½•ç½ªï¼Ÿ\n",
        "23.\tä½ æœ‰å°è±¡æ·±åˆ»çš„è´ªè…æ¡ˆä»¶å—ï¼Ÿ\n",
        "24.\tå¼ ä¸‰å‘æŸé«˜æ ¡æ‹›ç”Ÿäººå‘˜åˆ˜æŸé€2ä¸‡å…ƒï¼Œå¸Œæœ›åˆ˜æŸåœ¨æ‹›ç”Ÿæ—¶å½•å–è‡ªå·±çš„å¥³å„¿ã€‚é­åˆ°åˆ˜æŸå©‰æ‹’åï¼Œå¼ ä¸‰åˆæ‰¾åˆ°åˆ˜æŸçš„å¦»å­ï¼Œå…¶å¦»å­ç§è‡ªæŠŠé’±æ”¶ä¸‹äº†ã€‚å¼ ä¸‰æ˜¯å¦æ„æˆè¡Œè´¿ç½ªï¼Ÿ\n",
        "25.\tæŸå»ºç­‘å…¬å¸ï¼ˆä¸å…·æœ‰å»ºè®¾éš§é“çš„èµ„è´¨ï¼‰è€æ¿å¼ ä¸‰æ˜¯ç‹æŸçš„åŒå­¦ï¼Œå¼ ä¸‰æ‰¾åˆ°ç‹æŸï¼Œå¸Œæœ›ç‹æŸå‘å…¶çˆ¶ï¼ˆæŸåŸå»ºå±€å±€é•¿ï¼‰è¯´è¯´æƒ…ï¼ŒæŠŠéš§é“å·¥ç¨‹å‘åŒ…ç»™è‡ªå·±ï¼Œå¹¶ç»™äº†ç‹æŸä»·å€¼50ä¸‡å…ƒå»éŸ©å›½æ•´å®¹çš„è´µå®¾å¡åŠå¾€è¿”æœºç¥¨ã€‚ç‹æŸå‘çˆ¶äº²è¯´äº†æ­¤äº‹ï¼Œå¹¶å‡ºç¤ºäº†æœºç¥¨å’Œè´µå®¾å¡ã€‚å…¶çˆ¶å°†å·¥ç¨‹å‘åŒ…ç»™äº†å¼ ä¸‰ã€‚ç‹æŸæ˜¯å¦æ„æˆåˆ©ç”¨å½±å“åŠ›å—è´¿ç½ªï¼Ÿ\n",
        "è¯·ä½ åŸºäºè§†é¢‘å†…å®¹ï¼Œå¼€å§‹å›ç­”é—®é¢˜ï¼Œå›ç­”æ ¼å¼æ˜¯å…ˆåˆ—å‡ºé—®é¢˜ï¼Œå†ç»™å‡ºç­”æ¡ˆã€‚å¯¹äºæ— æ³•ä»è§†é¢‘ä¸­æ¨æ–­å‡ºç­”æ¡ˆçš„é—®é¢˜ï¼Œè¯·å›ç­”è§†é¢‘å¹¶æœªæåŠæœ‰å…³çš„çŸ¥è¯†ç‚¹ï¼Œæ— æ³•ä½œç­”ã€‚\n",
        "\"\"\"\n",
        "contents = [video_file, prompt]\n",
        "\n",
        "print(\"Making LLM inference request...\")\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=stream,\n",
        ")\n",
        "print(\"Successfully generating reponse:\")\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9d345e9868"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Docs on ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-gemini/cookbook/blob/main/quickstarts\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />More notebooks in the Cookbook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91AeeGO1ncU"
      },
      "source": [
        "## [optional] Show the conversation\n",
        "\n",
        "This section displays the conversation received from Google AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yoL3p3KPylFW"
      },
      "outputs": [],
      "source": [
        "# @title Show the conversation, in colab.\n",
        "import mimetypes\n",
        "\n",
        "def show_file(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "\n",
        "    if drive_id := file_data.get(\"drive_id\", None):\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        name = path\n",
        "        # data = path.read_bytes()\n",
        "        kwargs = {\"filename\": path}\n",
        "    elif url := file_data.get(\"url\", None):\n",
        "        name = url\n",
        "        kwargs = {\"url\": url}\n",
        "        # response = requests.get(url)\n",
        "        # data = response.content\n",
        "    elif data := file_data.get(\"inline_data\", None):\n",
        "        name = None\n",
        "        kwargs = {\"data\": data}\n",
        "    elif name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files to \"\n",
        "                'Colab using the file manager (\"ğŸ“ Files\"in the left toolbar)'\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "        print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "        return\n",
        "\n",
        "    format = mimetypes.guess_extension(mime_type).strip(\".\")\n",
        "    if mime_type.startswith(\"image/\"):\n",
        "        image = IPython.display.Image(**kwargs, width=256)\n",
        "        IPython.display.display(image)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if mime_type.startswith(\"audio/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Audio(**kwargs)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    if mime_type.startswith(\"video/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Video(**kwargs, mimetype=mime_type)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "\n",
        "\n",
        "for content in gais_contents:\n",
        "    if role := content.get(\"role\", None):\n",
        "        print(\"Role:\", role, \"\\n\")\n",
        "\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if text := part.get(\"text\", None):\n",
        "            print(text, \"\\n\")\n",
        "\n",
        "        elif file_data := part.get(\"file_data\", None):\n",
        "            show_file(file_data)\n",
        "\n",
        "    print(\"-\" * 80, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}